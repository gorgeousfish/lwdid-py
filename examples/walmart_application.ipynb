{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart Application: Effects of Walmart Opening on Local Retail Employment\n",
    "\n",
    "This notebook replicates the empirical analysis from Lee and Wooldridge (2025),\n",
    "\"A Simple Transformation Approach to Difference-in-Differences Estimation\n",
    "for Panel Data\" (SSRN 4516518), Section 6.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "- **Source**: Brown and Butts (2025), based on County Business Patterns (CBP) data\n",
    "- **Panel**: 1,280 counties over 23 years (1977-1999)\n",
    "- **Treatment**: First Walmart store opening in a county\n",
    "- **Outcome**: Log county-level retail employment\n",
    "\n",
    "## Reference Results (Table A4)\n",
    "\n",
    "Rolling IPWRA with Heterogeneous Trends:\n",
    "- ATT(0)  = 0.007 (SE=0.004)\n",
    "- ATT(1)  = 0.032 (SE=0.005)\n",
    "- ATT(2)  = 0.025 (SE=0.006)\n",
    "- ATT(3)  = 0.021 (SE=0.007)\n",
    "- ATT(4)  = 0.018 (SE=0.009)\n",
    "- ATT(5)  = 0.017 (SE=0.010)\n",
    "- ATT(6)  = 0.019 (SE=0.012)\n",
    "- ATT(7)  = 0.036 (SE=0.013)\n",
    "- ATT(8)  = 0.041 (SE=0.016)\n",
    "- ATT(9)  = 0.041 (SE=0.019)\n",
    "- ATT(10) = 0.037 (SE=0.023)\n",
    "- ATT(11) = 0.018 (SE=0.030)\n",
    "- ATT(12) = 0.017 (SE=0.036)\n",
    "- ATT(13) = 0.047 (SE=0.053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lwdid import lwdid\n",
    "\n",
    "warnings.formatwarning = lambda msg, cat, *a, **kw: f'{cat.__name__}: {msg}\\n'\n",
    "\n",
    "print(\"lwdid package loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Descriptive Statistics\n",
    "\n",
    "Load the Walmart data and verify descriptive statistics match Table 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/walmart.csv')\n",
    "\n",
    "print(f\"Data shape: {df.shape[0]:,} observations, {df.shape[1]} variables\")\n",
    "print(f\"Counties: {df['fips'].nunique():,}\")\n",
    "print(f\"Years: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"Observations per county: {df.groupby('fips').size().unique()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment cohort distribution (Table 2)\n",
    "print(\"Treatment Cohort Distribution (Table 2)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cohort_dist = df.groupby('g')['fips'].nunique().sort_index()\n",
    "n_never_treated = cohort_dist.get(np.inf, 0)\n",
    "n_treated = cohort_dist[cohort_dist.index != np.inf].sum()\n",
    "\n",
    "print(f\"Treated counties: {n_treated}\")\n",
    "print(f\"Never-treated counties: {n_never_treated}\")\n",
    "print(f\"Treatment cohort range: 1986 - 1999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify descriptive statistics (Table 2)\n",
    "stats = {\n",
    "    'log(Retail Employment)': ('log_retail_emp', 7.754502),\n",
    "    'Share Poverty (above)': ('share_pop_poverty_78_above', 0.8470385),\n",
    "    'Share in Manufacturing': ('share_pop_ind_manuf', 0.0998018),\n",
    "    'Share HS Graduate': ('share_school_some_hs', 0.092258),\n",
    "}\n",
    "\n",
    "print(f\"{'Variable':<30} {'Data Mean':>12} {'Paper Mean':>12} {'Match':>8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "all_match = True\n",
    "for name, (col, paper_val) in stats.items():\n",
    "    data_val = df[col].mean()\n",
    "    match = abs(data_val - paper_val) < 0.001\n",
    "    all_match = all_match and match\n",
    "    match_str = \"\\u2713\" if match else \"\\u2717\"\n",
    "    print(f\"{name:<30} {data_val:>12.6f} {paper_val:>12.6f} {match_str:>8}\")\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nAll descriptive statistics match Table 2 \\u2713\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some statistics do not match exactly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Define the estimation wrapper and WATT (Weighted Average Treatment Effect on the Treated) computation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rolling_ipwra(df, rolling_method, controls, control_group='not_yet_treated',\n",
    "                           include_pretreatment=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Estimate ATT using Rolling IPWRA method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Panel data\n",
    "    rolling_method : str\n",
    "        'demean' or 'detrend'\n",
    "    controls : list\n",
    "        Control variables\n",
    "    control_group : str\n",
    "        'never_treated', 'not_yet_treated', or 'all_others'\n",
    "    include_pretreatment : bool\n",
    "        Whether to compute pre-treatment effects (set False inside bootstrap for speed)\n",
    "    verbose : bool\n",
    "        Whether to print progress info\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LWDIDResults\n",
    "        Estimation results\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Estimating Rolling IPWRA with {rolling_method} (control: {control_group})...\")\n",
    "\n",
    "    results = lwdid(\n",
    "        data=df,\n",
    "        y='log_retail_emp',\n",
    "        ivar='fips',\n",
    "        tvar='year',\n",
    "        gvar='g',\n",
    "        rolling=rolling_method,\n",
    "        estimator='ipwra',\n",
    "        controls=controls,\n",
    "        control_group=control_group,\n",
    "        aggregate='none',\n",
    "        alpha=0.05,\n",
    "        include_pretreatment=include_pretreatment,\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_watt(results, df):\n",
    "    \"\"\"\n",
    "    Compute Weighted Average Treatment Effects on the Treated (WATT) by event time.\n",
    "\n",
    "    WATT(r) = sum_g w(g,r) * ATT(g, g+r)\n",
    "    where w(g,r) = N_g / N_Gr is the share of treated units in cohort g.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : LWDIDResults\n",
    "        Estimation results with cohort-time effects\n",
    "    df : pd.DataFrame\n",
    "        Original data for computing weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        WATT by event time\n",
    "    \"\"\"\n",
    "    att_ct = results.att_by_cohort_time.copy()\n",
    "\n",
    "    if att_ct is None or len(att_ct) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get cohort sizes for weighting\n",
    "    cohort_sizes = df[df['g'] != np.inf].groupby('g')['fips'].nunique().to_dict()\n",
    "\n",
    "    # Add weights\n",
    "    att_ct['weight'] = att_ct['cohort'].map(cohort_sizes)\n",
    "    att_ct['weight'] = att_ct['weight'].fillna(0)\n",
    "\n",
    "    # Aggregate by event time\n",
    "    watt_list = []\n",
    "\n",
    "    for event_time in sorted(att_ct['event_time'].unique()):\n",
    "        subset = att_ct[att_ct['event_time'] == event_time].copy()\n",
    "        subset = subset[subset['att'].notna()]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "\n",
    "        # Normalize weights\n",
    "        total_weight = subset['weight'].sum()\n",
    "        if total_weight == 0:\n",
    "            continue\n",
    "\n",
    "        subset['norm_weight'] = subset['weight'] / total_weight\n",
    "\n",
    "        # Weighted ATT\n",
    "        watt = (subset['att'] * subset['norm_weight']).sum()\n",
    "\n",
    "        # Weighted SE (conservative: assumes independence)\n",
    "        watt_se = np.sqrt((subset['se']**2 * subset['norm_weight']**2).sum())\n",
    "\n",
    "        # Number of cohorts contributing\n",
    "        n_cohorts = len(subset)\n",
    "        n_total = subset['n_treated'].sum() + subset['n_control'].sum()\n",
    "\n",
    "        watt_list.append({\n",
    "            'event_time': int(event_time),\n",
    "            'watt': watt,\n",
    "            'se': watt_se,\n",
    "            'ci_lower': watt - 1.96 * watt_se,\n",
    "            'ci_upper': watt + 1.96 * watt_se,\n",
    "            'n_cohorts': n_cohorts,\n",
    "            'n_total': n_total,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(watt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bootstrap Standard Errors\n",
    "\n",
    "Define the cluster bootstrap procedure for computing WATT standard errors (paper-style: bootstrap over units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bootstrap_resample_units(df, ivar, seed, rep):\n",
    "    \"\"\"\n",
    "    Cluster bootstrap at the unit level (resample units with replacement).\n",
    "    Duplicated units are assigned new synthetic unit IDs to keep (ivar, tvar) unique.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed + rep)\n",
    "    unit_to_idx = df.groupby(ivar, sort=False).indices\n",
    "    unit_ids = np.array(list(unit_to_idx.keys()))\n",
    "    sampled_ids = rng.choice(unit_ids, size=len(unit_ids), replace=True)\n",
    "\n",
    "    idx_arrays = [unit_to_idx[u] for u in sampled_ids]\n",
    "    boot_idx = np.concatenate(idx_arrays)\n",
    "\n",
    "    rep_counts = [len(unit_to_idx[u]) for u in sampled_ids]\n",
    "    new_unit_ids = np.repeat(np.arange(len(sampled_ids)), rep_counts)\n",
    "\n",
    "    boot_df = df.iloc[boot_idx].copy()\n",
    "    boot_df[ivar] = new_unit_ids\n",
    "    return boot_df\n",
    "\n",
    "\n",
    "def compute_watt_bootstrap_se(df, rolling_method, controls, control_group,\n",
    "                               *, n_bootstrap=100, seed=12345):\n",
    "    \"\"\"\n",
    "    Compute WATT and bootstrap SE (paper-style: bootstrap reps over units).\n",
    "\n",
    "    This is computationally expensive: it re-runs the full staggered pipeline\n",
    "    n_bootstrap times. Enable only when you explicitly want paper-style SEs.\n",
    "    \"\"\"\n",
    "    # Point estimate on original sample (no pre-treatment needed)\n",
    "    base_results = estimate_rolling_ipwra(\n",
    "        df, rolling_method, controls, control_group=control_group,\n",
    "        include_pretreatment=False, verbose=False\n",
    "    )\n",
    "    watt_point = compute_watt(base_results, df)\n",
    "    if len(watt_point) == 0:\n",
    "        return watt_point\n",
    "\n",
    "    event_times = watt_point['event_time'].tolist()\n",
    "    rep_matrix = {et: [] for et in event_times}\n",
    "\n",
    "    for b in range(n_bootstrap):\n",
    "        if (b + 1) % 10 == 0 or b == 0:\n",
    "            print(f\"  Bootstrap rep {b + 1}/{n_bootstrap}...\")\n",
    "        boot_df = _bootstrap_resample_units(df, ivar='fips', seed=seed, rep=b)\n",
    "        boot_results = estimate_rolling_ipwra(\n",
    "            boot_df, rolling_method, controls, control_group=control_group,\n",
    "            include_pretreatment=False, verbose=False\n",
    "        )\n",
    "        boot_watt = compute_watt(boot_results, boot_df)\n",
    "\n",
    "        for et in event_times:\n",
    "            vals = boot_watt.loc[boot_watt['event_time'] == et, 'watt'].values\n",
    "            rep_matrix[et].append(float(vals[0]) if len(vals) else np.nan)\n",
    "\n",
    "    # Replace SE/CI with bootstrap-based values\n",
    "    se_boot = []\n",
    "    for et in event_times:\n",
    "        arr = np.asarray(rep_matrix[et], dtype=float)\n",
    "        arr = arr[np.isfinite(arr)]\n",
    "        if len(arr) < 2:\n",
    "            se_boot.append(np.nan)\n",
    "        else:\n",
    "            se_boot.append(float(np.std(arr, ddof=1)))\n",
    "\n",
    "    watt_point = watt_point.copy()\n",
    "    watt_point['se'] = se_boot\n",
    "    watt_point['ci_lower'] = watt_point['watt'] - 1.96 * watt_point['se']\n",
    "    watt_point['ci_upper'] = watt_point['watt'] + 1.96 * watt_point['se']\n",
    "    watt_point['se_method'] = f'bootstrap({n_bootstrap})'\n",
    "    return watt_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Control Variables\n",
    "\n",
    "Control variables matching Table 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = [\n",
    "    'share_pop_poverty_78_above',  # Share above poverty line\n",
    "    'share_pop_ind_manuf',          # Share in manufacturing\n",
    "    'share_school_some_hs',         # Share with HS education\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rolling IPWRA with Demeaning\n",
    "\n",
    "Table A4 Rolling IPWRA (demean) column uses `control_group='all_others'`, where the treatment\n",
    "indicator is defined as 1{g_i=g} and the control group includes all non-cohort units (including\n",
    "already-treated ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_demean = estimate_rolling_ipwra(df, 'demean', controls, control_group='all_others')\n",
    "\n",
    "print(\"\\nCohort-Time ATT Estimates (first 10 rows):\")\n",
    "results_demean.att_by_cohort_time.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Rolling IPWRA with Detrending (Heterogeneous Trends)\n",
    "\n",
    "The heterogeneous trends (detrend) column uses the standard staggered DiD control group:\n",
    "not-yet-treated + never-treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_detrend = estimate_rolling_ipwra(df, 'detrend', controls, control_group='not_yet_treated')\n",
    "\n",
    "print(\"\\nCohort-Time ATT Estimates (first 10 rows):\")\n",
    "results_detrend.att_by_cohort_time.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Weighted ATT by Event Time\n",
    "\n",
    "The paper uses bootstrap SE (100 reps) for WATT standard errors and confidence intervals.\n",
    "\n",
    "Set `WALMART_FAST=1` environment variable to skip bootstrap and use analytical SE (for debugging only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_bootstrap = os.getenv('WALMART_FAST', '0') == '1'\n",
    "\n",
    "if skip_bootstrap:\n",
    "    print(\"[FAST MODE] Skipping bootstrap, using analytical SE (debug only)\")\n",
    "    watt_demean = compute_watt(results_demean, df)\n",
    "    watt_detrend = compute_watt(results_detrend, df)\n",
    "else:\n",
    "    reps = int(os.getenv('WALMART_WATT_BOOTSTRAP_REPS', '100'))\n",
    "    seed = int(os.getenv('WALMART_WATT_BOOTSTRAP_SEED', '12345'))\n",
    "    print(f\"Bootstrap WATT SE (paper config): reps={reps}, seed={seed}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\\nDemeaning bootstrap:\")\n",
    "    watt_demean = compute_watt_bootstrap_se(\n",
    "        df, 'demean', controls, control_group='all_others',\n",
    "        n_bootstrap=reps, seed=seed\n",
    "    )\n",
    "    print(\"\\nDetrending bootstrap:\")\n",
    "    watt_detrend = compute_watt_bootstrap_se(\n",
    "        df, 'detrend', controls, control_group='not_yet_treated',\n",
    "        n_bootstrap=reps, seed=seed\n",
    "    )\n",
    "\n",
    "print(\"\\nWATT with Demeaning:\")\n",
    "print(watt_demean.to_string(index=False))\n",
    "\n",
    "print(\"\\nWATT with Detrending:\")\n",
    "print(watt_detrend.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Paper Results (Table A4)\n",
    "\n",
    "Compare estimated WATT with the reference values from Table A4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper reference values (Table A4, last column: Rolling IPWRA with Het. Trends)\n",
    "paper_detrend = {\n",
    "    0: (0.007, 0.004), 1: (0.032, 0.005), 2: (0.025, 0.006),\n",
    "    3: (0.021, 0.007), 4: (0.018, 0.009), 5: (0.017, 0.010),\n",
    "    6: (0.019, 0.012), 7: (0.036, 0.013), 8: (0.041, 0.016),\n",
    "    9: (0.041, 0.019), 10: (0.037, 0.023), 11: (0.018, 0.030),\n",
    "    12: (0.017, 0.036), 13: (0.047, 0.053),\n",
    "}\n",
    "\n",
    "# Paper reference values for demeaning (Table A4, column 3)\n",
    "paper_demean = {\n",
    "    0: (0.018, 0.004), 1: (0.045, 0.004), 2: (0.038, 0.004),\n",
    "    3: (0.032, 0.004), 4: (0.031, 0.004), 5: (0.036, 0.005),\n",
    "    6: (0.040, 0.005), 7: (0.054, 0.006), 8: (0.062, 0.008),\n",
    "    9: (0.063, 0.010), 10: (0.081, 0.013), 11: (0.083, 0.018),\n",
    "    12: (0.080, 0.026), 13: (0.107, 0.039),\n",
    "}\n",
    "\n",
    "# Compare detrend results\n",
    "print(\"Rolling IPWRA with Detrending (Heterogeneous Trends)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'r':>3} | {'Python':>10} | {'Paper':>10} | {'Diff':>10} | {'Rating':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "detrend_diffs = []\n",
    "for _, row in watt_detrend.iterrows():\n",
    "    r = int(row['event_time'])\n",
    "    if r in paper_detrend:\n",
    "        paper_att, _ = paper_detrend[r]\n",
    "        diff = row['watt'] - paper_att\n",
    "        detrend_diffs.append(abs(diff))\n",
    "        rating = \"Close\" if abs(diff) < 0.005 else (\"Near\" if abs(diff) < 0.01 else \"Far\")\n",
    "        print(f\"{r:>3} | {row['watt']:>10.4f} | {paper_att:>10.4f} | {diff:>+10.4f} | {rating:>10}\")\n",
    "\n",
    "if detrend_diffs:\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Mean absolute difference: {np.mean(detrend_diffs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare demean results\n",
    "print(\"Rolling IPWRA with Demeaning\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'r':>3} | {'Python':>10} | {'Paper':>10} | {'Diff':>10} | {'Rating':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "demean_diffs = []\n",
    "for _, row in watt_demean.iterrows():\n",
    "    r = int(row['event_time'])\n",
    "    if r in paper_demean:\n",
    "        paper_att, _ = paper_demean[r]\n",
    "        diff = row['watt'] - paper_att\n",
    "        demean_diffs.append(abs(diff))\n",
    "        rating = \"Close\" if abs(diff) < 0.01 else (\"Near\" if abs(diff) < 0.03 else \"Far\")\n",
    "        print(f\"{r:>3} | {row['watt']:>10.4f} | {paper_att:>10.4f} | {diff:>+10.4f} | {rating:>10}\")\n",
    "\n",
    "if demean_diffs:\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Mean absolute difference: {np.mean(demean_diffs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Event Study Visualization (Figure 1)\n",
    "\n",
    "Generate event study plots similar to Figure 1 in the paper.\n",
    "Post-treatment uses bootstrap SE (consistent with the paper); pre-treatment uses analytical SE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lwdid.staggered.aggregation import aggregate_to_event_time, event_time_effects_to_dataframe\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "for idx, (results_obj, watt_post, panel_title) in enumerate([\n",
    "    (results_demean, watt_demean, '(b) Rolling IPWRA with unit-specific demeaning'),\n",
    "    (results_detrend, watt_detrend, '(c) Rolling IPWRA with unit-specific detrending'),\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Pre-treatment: aggregate from att_pre_treatment (analytical SE)\n",
    "    pre_plot = pd.DataFrame()\n",
    "    if results_obj.include_pretreatment and results_obj.att_pre_treatment is not None:\n",
    "        pre_ct = results_obj.att_pre_treatment.copy()\n",
    "        pre_effects = aggregate_to_event_time(\n",
    "            pre_ct, results_obj.cohort_sizes, alpha=0.05, df_strategy='conservative'\n",
    "        )\n",
    "        pre_plot = event_time_effects_to_dataframe(pre_effects)\n",
    "        pre_plot = pre_plot[pre_plot['event_time'] < 0].sort_values('event_time')\n",
    "\n",
    "    # Post-treatment: use WATT (bootstrap SE or analytical SE)\n",
    "    post_plot = watt_post[watt_post['event_time'] >= 0].copy().sort_values('event_time')\n",
    "\n",
    "    # Pre-treatment (blue error bars)\n",
    "    if len(pre_plot) > 0:\n",
    "        ax.errorbar(\n",
    "            pre_plot['event_time'], pre_plot['att'],\n",
    "            yerr=[pre_plot['att'] - pre_plot['ci_lower'], pre_plot['ci_upper'] - pre_plot['att']],\n",
    "            fmt='o-', color='steelblue', capsize=2, markersize=4,\n",
    "            linewidth=1.2, label='Pre-treatment',\n",
    "        )\n",
    "\n",
    "    # Post-treatment (red error bars)\n",
    "    if len(post_plot) > 0:\n",
    "        ax.errorbar(\n",
    "            post_plot['event_time'], post_plot['watt'],\n",
    "            yerr=1.96 * post_plot['se'],\n",
    "            fmt='o-', color='firebrick', capsize=2, markersize=4,\n",
    "            linewidth=1.2, label='Post-treatment',\n",
    "        )\n",
    "\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.6, alpha=0.7)\n",
    "    ax.axvline(x=-0.5, color='gray', linestyle=':', linewidth=0.8, alpha=0.5)\n",
    "    ax.set_xlabel('Time To Treatment', fontsize=10)\n",
    "    ax.set_ylabel('WATT', fontsize=10)\n",
    "    ax.set_title(panel_title, fontsize=11)\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('walmart_event_study.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved to: walmart_event_study.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key Findings (Detrending - Heterogeneous Trends):\")\n",
    "if len(watt_detrend) > 0:\n",
    "    post_watt = watt_detrend[watt_detrend['event_time'] >= 0]\n",
    "    if len(post_watt) > 0:\n",
    "        att_0 = post_watt[post_watt['event_time'] == 0]['watt'].values\n",
    "        att_1 = post_watt[post_watt['event_time'] == 1]['watt'].values\n",
    "\n",
    "        if len(att_0) > 0:\n",
    "            print(f\"  ATT(0) = {att_0[0]:.4f} (Instantaneous effect)\")\n",
    "        if len(att_1) > 0:\n",
    "            print(f\"  ATT(1) = {att_1[0]:.4f} (One year after opening)\")\n",
    "            pct_effect = (np.exp(att_1[0]) - 1) * 100\n",
    "            print(f\"         = {pct_effect:.1f}% increase in retail employment\")\n",
    "\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  The heterogeneous trends estimator shows more modest effects\")\n",
    "print(\"  compared to estimators that don't account for county-specific trends.\")\n",
    "print(\"  This suggests pre-existing trends may have inflated earlier estimates.\")\n",
    "print()\n",
    "print(\"Replication Analysis:\")\n",
    "print(\"  1. Detrend results closely match paper Table A4 (last column)\")\n",
    "print(\"  2. Demean results with control_group='all_others' closely match Table A4 (column 3)\")\n",
    "print(\"  3. Key qualitative findings consistent with paper:\")\n",
    "print(\"     - Detrending produces smaller, more conservative estimates\")\n",
    "print(\"     - Pre-treatment trends are flatter with detrending\")\n",
    "print(\"     - Effect of Walmart opening is positive but modest (~3%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
